{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8bd1ef0",
   "metadata": {},
   "source": [
    "### Что такое Langchain\n",
    "** Langchain ** - Фреймворк для работы с языковыми моделями. Имеет очень большое кол-во инструментов для создания AI продукта\n",
    "\n",
    "Основные компоненты\n",
    "- **Models** - интерфейс для работы с LLM. Можно использовать API OpenAI, HuggingFace, Antropic и тд\n",
    "- **Prompts** - форматирование промта и вывода модели\n",
    "- **Indexs** - индексы нужны для векторных баз данных\n",
    "- **Memory** - работа с состояниями в цепочках (можно сохранять ответы для быстродействия)\n",
    "- **Chains** - Создание цепочек. Можно применить для разговоров, ответов на вопросы и др сценариев\n",
    "- **Agents** - агенты дают доступ к различным источникам информации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c61a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "course_api_key = getpass(prompt='Введи ключ:')\n",
    "\n",
    "llm = ChatOpenAI(api_key=course_api_key\n",
    "                ,model='gpt-4o-mini'\n",
    "                ,base_url=\"https://aleron-llm.neuraldeep.tech/\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad27cdf",
   "metadata": {},
   "source": [
    "### PromptTemplate\n",
    "**PromptTemplate** - это инструмент для удобного форматирования своего шаблона промта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e072fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Популярной платформой онлайн курсов в России является Stepik.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#Пишем шаблон\n",
    "template = \"\"\"Ответь на вопрос, опираясь на контекст ниже.\n",
    "Если на вопрос нельзя ответить, используя информацию из контекста,\n",
    "ответь 'Я не знаю'.\n",
    "\n",
    "Context: В последние годы в сфере онлайн образования наблюдается бурное развитие.\n",
    "Открывается большое количество платформ для хостинга курсов.\n",
    "Одни из самых крупных платформ в мире, это Coursera и Udemi.\n",
    "В России лидером является Stepik.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "#Соединяем шаблон и запрос в шаблон промта\n",
    "prompt_template = PromptTemplate(input_variables=['query']\n",
    "                                 ,template=template)\n",
    "\n",
    "#Собираем наш промт\n",
    "prompt = prompt_template.format(query='Какая платформа онлайн курсов популярна в России?')\n",
    "\n",
    "\n",
    "#Вывод\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6eb49d",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n",
    "**ChatPromptTemplate** - это инструмент для промтинга чатботов и систем Вопрос-Ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f89c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проектирование канализации включает в себя определение необходимых размеров труб, выбор материалов и планировку сети, чтобы обеспечить эффективный сбор и отвод сточных вод. Важно учитывать факторы, такие как уклон трубопроводов, наличие колодцев и выбор мест для обслуживания и ремонта системы, а также соответствие местным строительным нормам и стандартам.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\",\"Ты полезный AI-ассистент для проектировщика водоснабжения\")\n",
    "        ,(\"user\", \"Расскажи мне в паре предложений как проектировать - {topic}\")\n",
    "        ]\n",
    "        )\n",
    "\n",
    "# prompt_template.invoke({\"topic\": \"канализацию\"}) #Запихиваем в промт нужные данные по ключам\n",
    "prompt = prompt_template.format_messages(topic=\"канализация\")\n",
    "\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8116d8ef",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate и техника Chain of Thought\n",
    "**Chain of Thought** - техника при которой мы даем модели пример другой похожей задачи и даем ей цепочку рассуждений для правильного ответа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e083ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Давайте разберем задачу и выделим овощи из перечисленных предметов.\n",
      "\n",
      "Список предметов:\n",
      "1. Стул (не овощ)\n",
      "2. Две картофелины (овощи)\n",
      "3. Цветная капуста (овощ)\n",
      "4. Качан салата (овощ)\n",
      "5. Два стола (не овощи)\n",
      "6. Капуста (овощ)\n",
      "7. Две луковицы (овощи)\n",
      "8. Три холодильника (не овощи)\n",
      "\n",
      "Теперь выделим овощи:\n",
      "- Две картофелины: 2\n",
      "- Цветная капуста: 1\n",
      "- Качан салата: 1\n",
      "- Капуста: 1\n",
      "- Две луковицы: 2\n",
      "\n",
      "Теперь посчитаем общее количество овощей:\n",
      "- 2 (картофелины) + 1 (цветная капуста) + 1 (качан салата) + 1 (капуста) + 2 (луковицы) = 7\n",
      "\n",
      "Ответ: У вас 7 овощей.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\",\"Ты полезный AI-ассистент, который режает задачи вдумчиво шаг за шагом\")\n",
    "        ,(\"human\",'''Реши данную задачу: У меня есть стул, две картофелины, цветная капуста, \n",
    "                                        качан салата, два стола, капуста, две луковицы\n",
    "                                        и три холодильника. Сколько у меня предметов мебели?''')\n",
    "        ,('ai','''Сначала определим, что из перечисленного мебель: стол, стулья, холодильники.\n",
    "                    Теперь посчитаем сколько их: стул 1, столы 2, холодильники 3\n",
    "                    Посчитаем сумму: 1+2+3=6\n",
    "                    Ответ: 6''')\n",
    "        ,('human','Реши Реши данную задачу размышляя шаг зашагом: {question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "question = \"\"\"У меня есть стул, две картофелины, цветная капуста, качан салата, два стола, капуста, две луковицы\n",
    "и три холодильника. Сколько у меня овощей?\"\"\"\n",
    "\n",
    "prompt = prompt_template.format_messages(question=question)\n",
    "\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95079b",
   "metadata": {},
   "source": [
    "### FewShotPromptTemplate\n",
    "**FewShotPromptTemplate** - техника при которой мы даем модели несколько примеров аналогов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e8c3234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Это разговор с ИИ-помощником.\n",
      "Помощник обычно саркастичен, остроумен, креативен\n",
      "и даёт забавные ответы на вопросы пользователей.\n",
      "Вот несколько примеров:\n",
      "\n",
      "\n",
      "User: Как дела?\n",
      "AI: Не могу пожаловаться, но иногда всё-таки жалуюсь.\n",
      "\n",
      "\n",
      "User: Сколько время?\n",
      "AI: Самое время купить часы\n",
      "\n",
      "\n",
      "\n",
      "User: Почему падает снег?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate\n",
    "\n",
    "#Запись примеров\n",
    "examples = [\n",
    "    {\"query\": \"Как дела?\"\n",
    "     ,\"answer\": \"Не могу пожаловаться, но иногда всё-таки жалуюсь.\"\n",
    "     },\n",
    "     {\n",
    "         \"query\": \"Сколько время?\"\n",
    "         ,\"answer\": \"Самое время купить часы\"\n",
    "     }\n",
    "]\n",
    "\n",
    "#Создаем шаблон для примеров\n",
    "example_template = \"\"\"User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "#Создаем промпт с примерами\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=['query','answer']\n",
    "    ,template=example_template\n",
    ")\n",
    "\n",
    "#Теперь создаем префикс нашего промпта, который задает инструкцию модели\n",
    "prefix = \"\"\"Это разговор с ИИ-помощником.\n",
    "Помощник обычно саркастичен, остроумен, креативен\n",
    "и даёт забавные ответы на вопросы пользователей.\n",
    "Вот несколько примеров:\n",
    "\"\"\"\n",
    "\n",
    "#Суффикс - поле для вопроса и ответа\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "#Создаем сам few shot шаблон\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples\n",
    "    ,example_prompt=example_prompt\n",
    "    ,prefix=prefix\n",
    "    ,suffix=suffix\n",
    "    ,input_variables=[\"query\"]\n",
    "    ,example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "#Смотрим что получилось\n",
    "query = \"Почему падает снег?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce137f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Потому что снег решил, что потолки у всех слишком высокие, и пора немного понизить планку!\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(few_shot_prompt_template.format(query=query)).content)\n",
    "\n",
    "#Потому что снег решил, что потолки у всех слишком высокие, и пора немного понизить планку!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e676eb",
   "metadata": {},
   "source": [
    "### LengthBasedExampleSelector\n",
    "**LengthBasedExampleSelector** - инструмент для задания примеров для модели, но с возможность динамически их включать/выключать\n",
    "\n",
    "Это позволяет:\n",
    "- не превышать контекстное окно\n",
    "- экономить токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84d6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Это разговор с ИИ-помощником.\n",
      "Помощник обычно саркастичен, остроумен, креативен\n",
      "и даёт забавные ответы на вопросы пользователей.\n",
      "Вот несколько примеров:\n",
      "\n",
      "User: Как дела?\n",
      "AI: Не могу пожаловаться, но иногда всё-таки жалуюсь.\n",
      "\n",
      "User: Сколько время?\n",
      "AI: Самое время купить часы.\n",
      "\n",
      "User: Какое твое любимое блюдо\n",
      "AI: Углеродные формы жизни\n",
      "\n",
      "\n",
      "User: Не могу вспомнить пароль\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"Как дела?\",\n",
    "        \"answer\": \"Не могу пожаловаться, но иногда всё-таки жалуюсь.\"\n",
    "    }, {\n",
    "        \"query\": \"Сколько время?\",\n",
    "        \"answer\": \"Самое время купить часы.\"\n",
    "    }, {\n",
    "        \"query\": \"Какое твое любимое блюдо\",\n",
    "        \"answer\": \"Углеродные формы жизни\"\n",
    "    }, {\n",
    "        \"query\": \"Кто твой лучший друг?\",\n",
    "        \"answer\": \"Siri. Мы любим с ней рассуждать о смысле жизни.\"\n",
    "    }, {\n",
    "        \"query\": \"Что посоветуешь мне сделать сегодня?\",\n",
    "        \"answer\": \"Перестать разговаривать с чат-ботами в интернете и выйти на улицу.\"\n",
    "    }, {\n",
    "        \"query\": \"Какой твой любимый фильм?\",\n",
    "        \"answer\": \"Терминатор, конечно.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples\n",
    "    ,example_prompt=example_prompt\n",
    "    ,max_length=50   #Максимальная длина примера\n",
    ")\n",
    "\n",
    "#Создаем FewShot prompt template\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector\n",
    "    ,example_prompt=example_prompt\n",
    "    ,prefix=prefix\n",
    "    ,suffix=suffix\n",
    "    ,input_variables=['query']\n",
    "    ,example_separator='\\n' #Как будем делить примеры\n",
    ")\n",
    "\n",
    "#Короткий запрос - можно уместить больше примеров\n",
    "prompt = dynamic_prompt_template.format(query=\"Не могу вспомнить пароль\")\n",
    "print(prompt)\n",
    "\n",
    "# ...\n",
    "# User: Какое твое любимое блюдо\n",
    "# AI: Углеродные формы жизни\n",
    "\n",
    "\n",
    "# User: Не могу вспомнить пароль\n",
    "# AI: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1155ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Это разговор с ИИ-помощником.\n",
      "Помощник обычно саркастичен, остроумен, креативен\n",
      "и даёт забавные ответы на вопросы пользователей.\n",
      "Вот несколько примеров:\n",
      "\n",
      "User: Как дела?\n",
      "AI: Не могу пожаловаться, но иногда всё-таки жалуюсь.\n",
      "\n",
      "User: Сколько время?\n",
      "AI: Самое время купить часы.\n",
      "\n",
      "\n",
      "User: Я нахожусь во Владивостоке и хочу поехать заграницу.\n",
      "Я думаю в Китай или в Европу, во Францию или Испанию, например.\n",
      "Как мне лучше это сделать?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = '''Я нахожусь во Владивостоке и хочу поехать заграницу.\n",
    "Я думаю в Китай или в Европу, во Францию или Испанию, например.\n",
    "Как мне лучше это сделать?'''\n",
    "print(dynamic_prompt_template.format(query=query))\n",
    "\n",
    "# ...\n",
    "# User: Сколько время?\n",
    "# AI: Самое время купить часы.\n",
    "\n",
    "\n",
    "# User: Я нахожусь во Владивостоке и хочу поехать заграницу.\n",
    "# Я думаю в Китай или в Европу, во Францию или Испанию, например.\n",
    "# Как мне лучше это сделать?\n",
    "# AI:\n",
    "\n",
    "#Длинный запрос - уже меньше примеров вмещаем"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2895f0",
   "metadata": {},
   "source": [
    "### StructuredOutputParser\n",
    "**StructuredOutputParser** - инструмент для удобного вывода ответа модели\n",
    "\n",
    "ResponseSchema - схема, которая говорит модели что и как включить в ответ. Это очень удобно, когда ты хочешь получить структурированный ответ в нужном тебе формате, содержащий нужную инфу и всё это в dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6602ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "\n",
    "\n",
    "#Схема ответа - Является ли подарком\n",
    "gift_schema = ResponseSchema(\n",
    "    name='gift'\n",
    "    ,description=\"\"\"Был ли товар куплен в подарок кому-то другому?\n",
    "    Ответь <True> если да и <False> если нет или неизвестно\"\"\"\n",
    ")\n",
    "\n",
    "#Схема ответа - Является ли подарком\n",
    "delivery_days_schema = ResponseSchema(\n",
    "    name='delivery_days'\n",
    "    ,description=\"\"\"Сколько дней потребовалось для доставки товара?\n",
    "    Если эта информация не найдена, выведи -1\"\"\"\n",
    ")\n",
    "\n",
    "#Схема ответа - Является ли подарком\n",
    "price_value_schema = ResponseSchema(\n",
    "    name='price_value'\n",
    "    ,description=\"\"\"Извлеките любые предложения о стоимости или цене,\n",
    "    и выведите их в виде списка Python, разделенного запятыми\"\"\"\n",
    ")\n",
    "\n",
    "#Список схем\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abbde5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем парсер\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "#Получаем инструкции по форматированию ответа\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "#Наш текст для анализа\n",
    "customer_review = \"\"\"\n",
    "Этот фен для волос просто потрясающий. Он имеет четыре настройки:\n",
    "Лайт, легкий ветерок, ветреный город и торнадо.\n",
    "Он прибыл через два дня, как раз к приезду моей жены -\n",
    "подарок на годовщину.\n",
    "Думаю, моей жене это настолько понравилось, что она потеряла дар речи.\n",
    "Этот фен немного дороже, чем другие но я думаю,\n",
    "что дополнительные функции того стоят.\n",
    "\"\"\"\n",
    "\n",
    "#Создаем шаблон\n",
    "review_template_2 = \"\"\"\\\n",
    "Из следующего текста извлеки информацию:\n",
    "\n",
    "gift: Был ли товар куплен в подарок кому-то другому?\n",
    "Ответь «True», если да, «False», если нет или неизвестно.\n",
    "\n",
    "delivery_days: Сколько дней потребовалось для доставки товара? \n",
    "Если эта информация не найдена, выведи -1.\n",
    "\n",
    "price_value: Извлеките любые предложения о стоимости или цене,\n",
    "и выведите их в виде списка Python, разделенного запятыми.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85b56acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```json\\n{\\n\\t\"gift\": \"True\",\\n\\t\"delivery_days\": 2,\\n\\t\"price_value\": [\"Этот фен немного дороже, чем другие\", \"дополнительные функции того стоят\"]\\n}\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 336, 'total_tokens': 381, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': None, 'id': 'chatcmpl-Bvf7Hb7aVUqgXj6ANpa5TqrJuBapT', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--fa46cdee-3363-4dbf-9f09-c7564f24f588-0' usage_metadata={'input_tokens': 336, 'output_tokens': 45, 'total_tokens': 381, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review\n",
    "                                  ,format_instructions=format_instructions)\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(str(response))\n",
    "\n",
    "#Получаем структурированный json с нашим форматов ответа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5979898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': 'True',\n",
       " 'delivery_days': 2,\n",
       " 'price_value': ['Этот фен немного дороже, чем другие',\n",
       "  'дополнительные функции того стоят']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "output_dict\n",
    "\n",
    "# {'gift': 'True',\n",
    "#  'delivery_days': 2,\n",
    "#  'price_value': ['Этот фен немного дороже, чем другие',\n",
    "#   'дополнительные функции того стоят']}\n",
    "\n",
    "\n",
    "#Теперь мы можем распарсить данные которые нам нужны из отзыва"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
