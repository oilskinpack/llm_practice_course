{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f51404",
   "metadata": {},
   "source": [
    "### InMemoryChatMessageHistory\n",
    "Контекст диалога будет храниться в оперативной памяти. После переапуска обнулится\n",
    "Для этого создается обертка RunnableWithMessageHistory\n",
    "\n",
    "При таком раскладе в конфигурацию добавляется **session ID** и (если нужно) **user ID**. Так можно хранить историю разных\n",
    "диалогов разных людей\n",
    "\n",
    "Для подгрузки в память можно юзать такие методы:\n",
    "- add_user_message() - для сообщений юзера\n",
    "- add_ai_message() - для сообщений AI\n",
    "- add_message()  - для любых сообщений\n",
    "- aadd_message() - для асинхронного добавления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a8b467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "course_api_key = getpass(prompt='Введите API-ключ полученный в боте')\n",
    "\n",
    "# инициализируем языковую модель\n",
    "llm = ChatOpenAI(api_key=course_api_key, model='gpt-4o-mini', \n",
    "                 base_url=\"https://aleron-llm.neuraldeep.tech/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "055a0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "#Создаем словарь который будет маппить session_id с историей\n",
    "store = {}\n",
    "\n",
    "#Функция возвращающая историю диалога по session_id\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5268f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем конфиг в который будем передавать session_id\n",
    "config = {\"configurable\" : {\"session_id\": \"1\"}}\n",
    "\n",
    "llm_with_history = RunnableWithMessageHistory(llm,get_session_history)\n",
    "\n",
    "#Теперь при вызове надо добавлять экземпляр config с нужной session_id\n",
    "answer = llm_with_history.invoke(\"Привет GPT! Меня зовут Виктор, как дела?\",config=config).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54e37e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Привет, Виктор! У меня всё хорошо, спасибо. Как дела у тебя? Чем могу помочь?\n",
      "Human: Привет GPT! Меня зовут Виктор, как дела?\n",
      "AI: Привет, Виктор! У меня всё хорошо, спасибо. Как дела у тебя? Чем могу помочь?\n"
     ]
    }
   ],
   "source": [
    "print(answer) #Привет, Виктор! У меня всё хорошо, спасибо. Как дела у тебя? Чем могу помочь?\n",
    "\n",
    "print(store['1'])\n",
    "# Human: Привет GPT! Меня зовут Виктор, как дела?\n",
    "# AI: Привет, Виктор! У меня всё хорошо, спасибо. Как дела у тебя? Чем могу помочь?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25c0cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Теперь при вызове надо добавлять экземпляр config с нужной session_id\n",
    "answer = llm_with_history.invoke(\"Как меня зовут?\",config=config).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3cf4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тебя зовут Виктор. Как я могу помочь тебе сегодня?\n"
     ]
    }
   ],
   "source": [
    "print(answer) #Тебя зовут Виктор. Как я могу помочь тебе сегодня?\n",
    "\n",
    "#Модель запомнила контекст и всё находится в сессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8c6a522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': InMemoryChatMessageHistory(messages=[])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Очистить историю диалога сессии можно методом clear()\n",
    "get_session_history('1').clear()\n",
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6940c88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello, World\n",
      "AI: Hi\n"
     ]
    }
   ],
   "source": [
    "#Добавим сообщения в сессию\n",
    "get_session_history('1').clear()\n",
    "get_session_history('1').add_user_message('Hello, World')\n",
    "get_session_history('1').add_ai_message('Hi')\n",
    "\n",
    "print(store['1'])\n",
    "\n",
    "# Human: Hello, World\n",
    "# AI: Hi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeddde7",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n",
    "Встраиваем историю в промпт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e2444e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "#Создаем шаблон для ассистента\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"Ты полезный ассистент, отлично разбирающийся в {topic}\")\n",
    "    ,MessagesPlaceholder(variable_name='chat_history')  #Заглушка куда будем поставлять историю диалога\n",
    "    ,('human', '{question}')\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0e925f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Собираем цепочку\n",
    "chain = prompt | llm\n",
    "\n",
    "#Добавляем сохранение истории взаимодействий\n",
    "chain_with_history = RunnableWithMessageHistory(chain\n",
    "                                                ,get_session_history\n",
    "                                                ,input_messages_key=\"question\"\n",
    "                                                ,history_messages_key='chat_history')\n",
    "\n",
    "#Выполняем запрос\n",
    "answer = chain_with_history.invoke({\"topic\": \"математика\", \"question\": \"Чему равен синус 30?\"}\n",
    "                          ,config= {\"configurable\" : {\"session_id\": \"2\"}}).content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803f82b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Синус 30 градусов равен \\( \\frac{1}{2} \\) или 0.5.\n"
     ]
    }
   ],
   "source": [
    "print(answer) #Синус 30 градусов равен \\( \\frac{1}{2} \\) или 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51cad7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#А теперь задаем второй вопрос, по контексту уже модель понимает что мы ходим\n",
    "answer = chain_with_history.invoke(\n",
    "    {\"topic\": \"математика\", \"question\": \"А чему равен косинус?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"2\"}}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314242b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Косинус 30 градусов равен \\( \\frac{\\sqrt{3}}{2} \\) или примерно 0.866.\n"
     ]
    }
   ],
   "source": [
    "print(answer) #Косинус 30 градусов равен \\( \\frac{\\sqrt{3}}{2} \\) или примерно 0.866.\n",
    "\n",
    "#Модель понимает контекст"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435893cd",
   "metadata": {},
   "source": [
    "### Минусы таких подходов\n",
    "\n",
    "Недостатки InMemoryChatMessageHistory:\n",
    "\n",
    "- История хранится в оперативной памяти. При интенсивных диалогах и большом количестве пользователей память может закончиться\n",
    "- При перезагрузке или сбое сервера - вся история исчезнет\n",
    "- Чем длиннее история, тем больше токенов придется подавать на вход модели\n",
    "- В случае платных моделей, это будет накладно по финансам\n",
    "- Контекстное окно моделей тоже ограничено по количеству токенов\n",
    "\n",
    "Для продакшен решений можно рассмотреть более практичные форматы хранения истории в Langchain, например, релизованы:\n",
    "\n",
    "- FileChatMessageHistory - сохраняет историю взаимодействия сообщений в файл.\n",
    "- RedisChatMessageHistory - сохраняет историю сообщений чата в базе данных Redis.\n",
    "- SQLChatMessageHistory - сохраняет историю сообщений чата в базе данных SQL.\n",
    "- MongoDBChatMessageHistory - в базе данных Mongo\n",
    "и многие другие - https://python.langchain.com/api_reference/community/chat_message_histories.html\n",
    "\n",
    "Например, это могло бы выглядеть так:\n",
    "\n",
    "engine = sqlalchemy.create_engine(\"sqlite:///database/chat_history.db\")\n",
    "\n",
    "SQLChatMessageHistory(session_id=session_id, connection=engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
